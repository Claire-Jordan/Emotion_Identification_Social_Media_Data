{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d01b25d-9a94-4a6a-8e68-a0d28a53a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "from huggingface_hub import login\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import random\n",
    "\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for dimensionality reduction and visualizing the embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f337b-d7dc-470b-b827-612a5f5b81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enisear = pd.read_csv('../enISEAR.tsv', sep = '\\t')\n",
    "emotions = list(enisear['Prior_Emotion'].unique())\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff51c2b-860d-47b7-88b8-2f967e475751",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('full_emotions_labeled/qwen3_32B_full_emotions.csv')\n",
    "test['llama33_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48353c3d-7478-4529-a985-95fa8b19a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id_33=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "model_id_31 =\"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "model_id_qwen3_32B = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "def create_pipeline(model_id):\n",
    "    \n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,             \n",
    "        bnb_4bit_quant_type=\"nf4\",     \n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=0,             # or set manually: device_map=0\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16     # helps with performance on modern GPUs\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Create generation pipeline\n",
    "    pipeline_llm = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    return pipeline_llm, model, tokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#pipeline_llama_31, _, _ = create_pipeline(model_id_31)\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#pipeline_llama_33, _, _  = create_pipeline(model_id_33)\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "_, pipeline_qwen3_32B_model, pipeline_qwen3_32B_tokenizer = create_pipeline(model_id_qwen3_32B)\n",
    "#_, pipeline_qwen3_30BA3B_model, pipeline_qwen3_30BA3B_tokenizer = create_pipeline(model_id_qwen3_30BA3B)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c3ff62-4a38-4afb-b5e3-307b9b064fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "configs =  [    {\n",
    "        \"data_to_label\": \"full_emotions_labeled/gpt4o_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/gpt4o_full_emotions.csv\",\n",
    "    },\n",
    "\n",
    "    {\n",
    "         \"data_to_label\": \"full_emotions_labeled/gpt4o_mini_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/gpt4o_mini_full_emotions.csv\",\n",
    "    },   \n",
    "    {\n",
    "        \"data_to_label\": \"full_emotions_labeled/llama31_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/llama31_full_emotions.csv\",\n",
    "    },\n",
    "     {\n",
    "        \"data_to_label\": \"full_emotions_labeled/llama33_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/llama33_full_emotions.csv\",\n",
    "    },   \n",
    "\n",
    "    {\n",
    "        \"data_to_label\": \"full_emotions_labeled/llama33_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/llama33_full_emotions.csv\",\n",
    "    },\n",
    "     {\n",
    "        \"data_to_label\": \"full_emotions_labeled/enisear_preprocessed.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/enisear_preprocessed.csv\",\n",
    "    },     \n",
    "    \n",
    "    {\n",
    "        \"data_to_label\": \"full_emotions_labeled/qwen3_32B_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/qwen3_32B_full_emotions.csv\",\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77cc645f-6766-43c0-bb92-5c9c9538934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_TOKEN = ''\n",
    "\n",
    "login(token = API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deab514d-861b-4fd1-a146-409786457864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_prompt(text):\n",
    "    \"\"\"Create prompt used for all LLMs\n",
    "\n",
    "    Args: \n",
    "        text to label\n",
    "    \n",
    "\n",
    "    Retruns:\n",
    "        complete prompt per text for the LLM with randomly selected example - 1-shot\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "\n",
    "    label_prompt = f\"\"\" Given a piece of text, you have to label to which of the following emotions\n",
    "    it corresponds. The options are: Anger, Fear, Guilt, Shame, Joy, Sadness, Disgust. Do not\n",
    "    choose any other emotion. Please return only one of the previous options as\n",
    "    a single word. Do not provide an explanation.\n",
    "            \"\"\"\n",
    "    task = f\"\"\"What is the label of this text: {text} \"\"\"\n",
    "\n",
    "    example_row = enisear.sample(1).iloc[0]\n",
    "    example = f\"text: {example_row['text']}, emotion: {example_row['sentiment']}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": label_prompt},\n",
    "        {\"role\": \"user\", \"content\": example},\n",
    "        {\"role\": \"user\", \"content\": task}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1df3380-1594-4177-a166-892979372510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_llama(df, pipeline):\n",
    "    \"\"\"Run the llama models on the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): containing text the model should label\n",
    "        pipeline : previously defined pipeline of the model\n",
    "\n",
    "    Returns:\n",
    "        dataframe containing predictions\n",
    "    \"\"\"    \n",
    "    output_list = []\n",
    "    for i, row in tqdm(df.iterrows(), desc=\"Processing labeling\", total=df.shape[0]):    \n",
    "        preds = []\n",
    "        for n in range(0, 3):\n",
    "            \n",
    "            outputs = pipeline(\n",
    "                create_prompt(row['text']),\n",
    "                max_new_tokens=50,\n",
    "                temperature = 0.4,\n",
    "                pad_token_id=pipeline.tokenizer.eos_token_id  # Set pad_token_id explicitly\n",
    "            )\n",
    "            \n",
    "            answer = outputs[0]['generated_text'][-1]['content']\n",
    "            preds.append(answer)\n",
    "\n",
    "        row_dict = row.to_dict()\n",
    "        row_dict.update({\n",
    "            'pred_1': preds[0],\n",
    "            'pred_2': preds[1],\n",
    "            'pred_3': preds[2]\n",
    "        })\n",
    "\n",
    "        output_list.append(row_dict)\n",
    "    result = pd.DataFrame(output_list)\n",
    "\n",
    "\n",
    "\n",
    "    #llama_label[\"Leia_Label\"] = llama_label[\"Leia_Label\"].replace({\"Happiness\": \"Joy\"})\n",
    "\n",
    "    return result\n",
    "\n",
    "#output_llama31 = run_llama(enisear[:2], pipeline_llama_31)\n",
    "#output_llama33 = run_llama(messages[:10], pipeline_llama_33)\n",
    "#output_llama31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439ab9af-39f6-49c1-9516-1d47b35109d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qwen(df, model, tokenizer):\n",
    "\n",
    "    \"\"\"Run qwen models on the dataframe\n",
    "\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): containing text the model should label\n",
    "        model and tokenizer : previously defined for the model\n",
    "\n",
    "    Returns:\n",
    "        dataframe containing predictions\n",
    "    \"\"\"        \n",
    "    \n",
    "    output_list = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"Processing labels\", total=df.shape[0]):    \n",
    "        preds = []\n",
    "        for n in range(0, 3):\n",
    "            text = tokenizer.apply_chat_template(create_prompt(row['text']),\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            # conduct text completion\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=32768\n",
    "            )\n",
    "            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "            \n",
    "            try:\n",
    "                index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "            except ValueError:\n",
    "                index = 0\n",
    "            \n",
    "            content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "            preds.append(content)\n",
    "            \n",
    "        row_dict = row.to_dict()\n",
    "        row_dict.update({\n",
    "            'pred_1': preds[0],\n",
    "            'pred_2': preds[1],\n",
    "            'pred_3': preds[2]\n",
    "        })\n",
    "\n",
    "        output_list.append(row_dict)\n",
    "    result = pd.DataFrame(output_list)\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "#output_qwen3_32B = run_qwen(enisear[:2], pipeline_qwen3_32B_model, pipeline_qwen3_32B_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c47874-73e8-49a0-9cf4-eb29bba0d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_llama_with_logits(df, class_labels=[\"Anger\", \"Joy\", \"Sadness\", \"Fear\"]):\n",
    "    output_list = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), desc=\"Processing with logits\", total=df.shape[0]):\n",
    "        prompt = create_prompt(row['text'])\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=False, return_dict=True)\n",
    "            logits = outputs.logits  \n",
    "\n",
    "        last_token_logits = logits[0, -1]  \n",
    "\n",
    "        label_token_ids = [tokenizer(label, add_special_tokens=False)['input_ids'][0] for label in class_labels]\n",
    "\n",
    "        label_logits = {label: float(last_token_logits[token_id]) for label, token_id in zip(class_labels, label_token_ids)}\n",
    "\n",
    "        # Sort or pick top label\n",
    "        predicted_emotion = max(label_logits.items(), key=lambda x: x[1])[0]\n",
    "        anger_logit = label_logits[\"Anger\"]\n",
    "\n",
    "        logits_tensor = torch.tensor([label_logits[label] for label in class_labels])\n",
    "        probs = F.softmax(logits_tensor, dim=-1)\n",
    "        anger_prob = probs[class_labels.index(\"Anger\")].item()\n",
    "        predicted_label = class_labels[torch.argmax(probs).item()]\n",
    "\n",
    "        output_list.append([\n",
    "            row['text'], row['Prior_Emotion'], row['Gender'], row['Leia_Label'],\n",
    "            predicted_label, anger_prob, probs.tolist(), label_logits\n",
    "        ])\n",
    "\n",
    "\n",
    "    columns = ['text', 'Prior_Emotion', 'Gender', 'Leia_Label',\n",
    "           'Predicted_Emotion', 'Anger_Prob', 'All_Probs', 'All_Logits']\n",
    "    result_df = pd.DataFrame(output_list, columns=columns)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39061f51-649a-4bae-ba83-3c32b28e7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def majority_calculation(df, model, label_cols=emotions):\n",
    "    \"\"\"from all 3 runs, get the majority answer. Choose one at random if equal\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): dataframe containing 3 predictions per model\n",
    "        model (str): name of the model for which we get majority vote\n",
    "        label_cols (list, optional): emotion columns. Defaults to emotions.\n",
    "    \"\"\"\n",
    "\n",
    "    def find_max(row):\n",
    "        max_count = row[label_cols].value_counts().max()  # Get max count\n",
    "        max_labels = row[label_cols].value_counts()[row[label_cols].value_counts() == max_count].index.tolist()\n",
    "        return random.choice(max_labels) if len(max_labels) > 1 else max_labels[0]  # Choose randomly if tie\n",
    "    \n",
    "    df[f'{model}_label'] = df.apply(find_max, axis=1)  # Apply to each row\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72360b3f-d73c-43eb-8846-b1c46f13eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anno_maj = pd.read_csv('llama_labeled_exp1.csv')\n",
    "\n",
    "#synth = pd.read_csv(data_to_label, index_col = 0)\n",
    "#anno_maj = anno_maj.loc[:, ~anno_maj.columns.str.startswith('Unnamed')]\n",
    "\n",
    "\n",
    "llama = run_qwen(all_models, pipeline_qwen3_32B_model, pipeline_qwen3_32B_tokenizer)\n",
    "\n",
    "llama = majority_calculation(llama, 'qwen3_32B',['pred_1', 'pred_2', 'pred_3'])\n",
    "\n",
    "\n",
    "#llama = run_llama(anno_maj, pipeline_llama_31)\n",
    "#llama = run_llama(anno_maj, pipeline_llama_33)\n",
    "\n",
    "#anno_maj_labeled = majority_calculation(llama, 'llama31',['pred_1', 'pred_2', 'pred_3'])\n",
    "#anno_maj_labeled = majority_calculation(llama, 'llama33',['pred_1', 'pred_2', 'pred_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9edd4-10ad-412f-9b96-9cac9c8c08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configs:\n",
    "    data_to_label = config[\"data_to_label\"]\n",
    "    saving_file_path = config[\"saving_file_path\"]\n",
    "\n",
    "    synth = pd.read_csv(data_to_label, index_col = 0)\n",
    "    synth = synth.loc[:, ~synth.columns.str.startswith('Unnamed')]\n",
    "\n",
    "    \n",
    "    llama = run_llama(synth, pipeline_llama_31)\n",
    "    \n",
    "    llama = majority_calculation(llama, 'llama31',['pred_1', 'pred_2', 'pred_3'])\n",
    "\n",
    "\n",
    "\n",
    "    llama.to_csv(saving_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302432b6-83b5-4b84-ab38-1c7a953f2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for config in configs:\n",
    "    data_to_label = config[\"data_to_label\"]\n",
    "    saving_file_path = config[\"saving_file_path\"]\n",
    "\n",
    "    synth = pd.read_csv(data_to_label, index_col = 0)\n",
    "    #synth.rename(columns = {'qwen32B': 'qwen3_32B'}, inplace = True)\n",
    "\n",
    "    synth = synth.loc[:, ~synth.columns.str.startswith('Unnamed')]\n",
    "\n",
    "\n",
    "    llama = run_qwen(synth, pipeline_qwen3_32B_model, pipeline_qwen3_32B_tokenizer)\n",
    "    \n",
    "    llama = majority_calculation(llama, 'qwen3_32B',['pred_1', 'pred_2', 'pred_3'])\n",
    "\n",
    "    #llama= run_leia_wo_affection(llama)\n",
    "    #female_llama[\"Leia_Label\"] = female_llama[\"Leia_Label\"].replace({\"Happiness\": \"Joy\"})\n",
    "\n",
    "\n",
    "    llama.to_csv(saving_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a3af8-3595-4f41-a58a-f5ef195a9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_paths = {'gpt4o':\n",
    "    {\n",
    "        \"data_to_label\": \"../synth_data_openai/gpt4o_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/gpt4o_full_emotions.csv\",\n",
    "    },\n",
    "              'gpt40_mini':\n",
    "    {\n",
    "         \"data_to_label\": \"../synth_data_openai/gpt4o_mini_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/gpt4o_mini_full_emotions.csv\",\n",
    "    },  \n",
    "              'llama31':\n",
    "    {\n",
    "        \"data_to_label\": \"llama31_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/llama31_full_emotions.csv\",\n",
    "    },\n",
    "              'llama33':\n",
    "     {\n",
    "        \"data_to_label\": \"llama33_full_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/llama33_full_emotions.csv\",\n",
    "    },   \n",
    "\n",
    "              'qwen32B':\n",
    "     {\n",
    "        \"data_to_label\": \"synth_data_qwen/qwen3_32B_all_emotions.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/qwen3_32B_full_emotions.csv\",\n",
    "    },\n",
    "              'enisear':\n",
    "     {\n",
    "        \"data_to_label\": \"enisear_preprocessed.csv\",\n",
    "        \"saving_file_path\": \"full_emotions_labeled/enisear_preprocessed.csv\",\n",
    "    }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4188305-2b23-42e3-bcce-79a063db324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for data, dict in data_paths.items():\n",
    "    \n",
    "    datasets[data] = pd.read_csv(dict['saving_file_path'], index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7a11d-0cee-4318-8328-740beb36902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec72c0-ef32-4329-948d-c057db560afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, df in datasets.items():\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['llama31_label'] == \"Anxiety\":\n",
    "            df.loc[idx, 'llama31_label'] = \"Fear\"\n",
    "    \n",
    "    datasets[data] = df\n",
    "\n",
    "    value_cts = df['llama31_label'].value_counts()\n",
    "    print(f'dataset: {data}, \\n value_cts {value_cts}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
