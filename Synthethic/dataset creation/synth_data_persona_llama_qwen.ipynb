{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d01b25d-9a94-4a6a-8e68-a0d28a53a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import random\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# For BERT\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a621596-6cd6-4b0c-bcd4-aa72cfbea9bd",
   "metadata": {},
   "source": [
    "# ISEAR preprocessing + Example generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fcb83a-4c9a-4d39-9be4-201067092e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enisear = pd.read_csv('../enISEAR.tsv', delimiter= '\\t')\n",
    "emotions = enisear['Prior_Emotion'].unique()\n",
    "enisear['Prior_Emotion'].nunique()\n",
    "#emotions\n",
    "len(enisear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a5b1d-e898-4dc4-98ac-98b4e094d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_equal_sample(df, endsize: int, rand_state: int):\n",
    "\n",
    "    \"\"\"function to extract a sample of equal size of text per emotion\n",
    "\n",
    "    Returns:\n",
    "        dictionary: containing equal size of samples per emotion\n",
    "    \"\"\"\n",
    "\n",
    "    sample_size_per_class = endsize // df['Prior_Emotion'].nunique()\n",
    "\n",
    "    # Stratified sampling\n",
    "    equal_sample = (\n",
    "        df.groupby('Prior_Emotion', group_keys=False)\n",
    "        .apply(lambda x: x.sample(sample_size_per_class, random_state=rand_state))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return equal_sample[['Sentence', 'Prior_Emotion', 'Gender']]\n",
    "\n",
    "enisear_examples = get_equal_sample(enisear, 7, 321)\n",
    "enisear_examples\n",
    "examples = []\n",
    "\n",
    "for idx, row in enisear_examples.iterrows():\n",
    "    text_for_example = f'''\n",
    "\"Prior_Emotion\": \"{row['Prior_Emotion']}\",\n",
    "\"Sentence\": \"{row['Sentence']}\",\n",
    "'''\n",
    "    examples.append({'emotion':row['Prior_Emotion'], 'Sentence':text_for_example})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d69797-545d-4071-9591-285a82b5736e",
   "metadata": {},
   "source": [
    "# Persona Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77cc645f-6766-43c0-bb92-5c9c9538934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_TOKEN = ''\n",
    "\n",
    "login(token = API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b964a12c-8e29-46b8-baa9-ac3a6c275738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_persona = load_dataset(\"proj-persona/PersonaHub\", \"persona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b41f426-1a99-48ad-aed4-5e964ba7e7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['persona'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_persona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafc40a-0edd-4234-9996-490153b253db",
   "metadata": {},
   "source": [
    "# Pipelines per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c826dd39-323c-4c1c-aa99-e1ca9f02b835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85ce5afb2684f1091d396e7414fb8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id_33=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "model_id_31 =\"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "model_id_qwen3_32B = \"Qwen/Qwen3-32B\"\n",
    "\n",
    "def create_pipeline(model_id):\n",
    "    \n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,             \n",
    "        bnb_4bit_quant_type=\"nf4\",     \n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=0,            \n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16     \n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    " \n",
    "    pipeline_llm = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    return pipeline_llm, model, tokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pipeline_llama_31, _, _ = create_pipeline(model_id_31)\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#pipeline_llama_33, _, _  = create_pipeline(model_id_33)\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#_, pipeline_qwen3_32B_model, pipeline_qwen3_32B_tokenizer = create_pipeline(model_id_qwen3_32B)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b575573f-1421-45f5-b257-158e1dd39e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fear', 'Shame', 'Guilt', 'Disgust', 'Sadness', 'Anger', 'Joy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = list(enisear['Prior_Emotion'].unique())\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4c6694-747d-4272-a8fa-1a2952155aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_prompt(tries):\n",
    "\n",
    "    \"\"\"Create prompt used for all LLMs\n",
    "\n",
    "    Args: \n",
    "        tries: number of data elements wanted per emotion\n",
    "    \n",
    "\n",
    "    Retruns:\n",
    "        complete prompt per text for the LLM with randomly selected example - 1-shot\n",
    "    \n",
    "    \"\"\"\n",
    "    persona_prompts = []\n",
    "\n",
    "    for _ in range(tries):\n",
    "        #random_persona = random.randint(0, 200000)\n",
    "        #persona = ds_persona['train'][random_persona]['persona']\n",
    "\n",
    "        for emotion in emotions:  # Loop over emotions\n",
    "\n",
    "            persona_prompt = {}\n",
    "\n",
    "            #  Set up the rules and persona\n",
    "            synth_isear_prompt = f\"\"\"\n",
    "            You are currently participating in a psychological research study on emotions.  You must complete the following sentence: \n",
    "            'I felt {emotion} when ...', describing an experience when you felt  {emotion}.\n",
    "            Do not change the pattern of the sentence. Do not change the provided emotion.\n",
    "            Your response must be elaborate and at least 30 words long. Do not reuse the example. Do not change the dictionary key format. \n",
    "            \"\"\"\n",
    "            \n",
    "            example = next((e['Sentence'] for e in examples if e['emotion'] == emotion), None)\n",
    "\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": synth_isear_prompt},\n",
    "        \n",
    "            {\"role\": \"user\", \"content\": f\"Example for emotion: {emotion}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example},\n",
    "        \n",
    "            {\"role\": \"user\", \"content\": 'Now, generate one response for this emotion. Follow the format strictly: {\"Prior_Emotion\": \"\", \"Sentence\": \"\"}. Strictly follow all provided instructions!'}\n",
    "            ]\n",
    "\n",
    "            #persona_prompt['persona'] = persona\n",
    "            persona_prompt['emotion'] = emotion  \n",
    "            persona_prompt['messages'] = messages\n",
    "\n",
    "\n",
    "            persona_prompts.append(persona_prompt)\n",
    "\n",
    "    return persona_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ede3b0-570d-488a-8e4e-0fdab975aa6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = create_prompt(143)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b204b-0084-4428-8b7d-ae561fd41693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de238df1-3edb-45ba-bc2b-52decd8ccb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing messages:   1%|          | 10/1001 [00:37<1:04:00,  3.88s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing messages: 100%|██████████| 1001/1001 [1:11:33<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_llama(messages, pipeline):\n",
    "    \"\"\"Run the llama models on the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): containing text the model should label\n",
    "        pipeline : previously defined pipeline of the model\n",
    "\n",
    "    Returns:\n",
    "        dataframe containing predictions\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "    for i in tqdm(messages, desc=\"Processing messages\"):    \n",
    "        outputs = pipeline(\n",
    "            i['messages'],\n",
    "            max_new_tokens=1000,\n",
    "            temperature = 1,\n",
    "            pad_token_id=pipeline.tokenizer.eos_token_id \n",
    "        )\n",
    "        \n",
    "        answer = outputs[0]['generated_text'][-1]['content']\n",
    "    \n",
    "        output_list.append(answer)\n",
    "    return output_list\n",
    "output_llama31 = run_llama(messages, pipeline_llama_31)\n",
    "#output_llama33 = run_llama(messages, pipeline_llama_33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81831d-8216-495a-a09d-f6b1b46f0e16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_qwen(messages, tokenizer, model):\n",
    "\n",
    "    \"\"\"Run qwen models on the dataframe\n",
    "\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): containing text the model should label\n",
    "        model and tokenizer : previously defined for the model\n",
    "\n",
    "    Returns:\n",
    "        dataframe containing predictions\n",
    "    \"\"\"    \n",
    "    output_list = []\n",
    "    \n",
    "    for i in tqdm(messages, desc=\"Processing messages\"):    \n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            i['messages'],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # conduct text completion\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=32768\n",
    "        )\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "        \n",
    "        # parsing thinking content\n",
    "        try:\n",
    "            # rindex finding 151668 (</think>)\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "        \n",
    "        content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "        output_list.append(content)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "#output_qwen3_32B = run_qwen(messages, pipeline_qwen3_32B_tokenizer, pipeline_qwen3_32B_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e8c2ed-f5e4-42de-aed6-2163edebfba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_entry(entry):\n",
    "    \"\"\"Normalizing all results to extract json format from string\n",
    "\n",
    "    Args:\n",
    "        entry (string): text from answer list of llm output\n",
    "\n",
    "    Returns:\n",
    "        json: entry in json format for easier handling\n",
    "    \"\"\"\n",
    "\n",
    "    entry = entry.replace('\\\\n', ' ').replace(\"\\\\\", '')  # Remove escape characters\n",
    "\n",
    "\n",
    "    entry = re.sub(r'\\(Note[^\\)]+\\)', '', entry)  # Remove unnecessary notes\n",
    "    if not entry.endswith('}'):\n",
    "        entry += '}'  # Fix missing closing bracket if needed\n",
    "\n",
    "    if not entry.startswith('{'):\n",
    "        entry = '{' + entry\n",
    "    return entry\n",
    "\n",
    "def fix_json(entry):\n",
    "    try:\n",
    "        return json.loads(entry) \n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            fixed_entry = ast.literal_eval(entry)  \n",
    "            return json.loads(json.dumps(fixed_entry)) \n",
    "        except (SyntaxError, ValueError):\n",
    "            return None  \n",
    "def fix_all(output_list):\n",
    "    \n",
    "\n",
    "    output_list_cleaned = [clean_entry(i) for i in output_list]\n",
    "\n",
    "    output_list_fixed = [fix_json(entry) for entry in output_list_cleaned if fix_json(entry) is not None]\n",
    "\n",
    "    return output_list_fixed, output_list_cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#output_qwen3_32B_fixed, output_qwen3_32B_cleaned = fix_all(output_qwen3_32B)\n",
    "output_llama31_fixed, output_llama31_cleaned = fix_all(output_llama31)\n",
    "#output_llama33_fixed, output_llama33_cleaned = fix_all(output_llama33)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81044f80-da10-40e6-9369-63cb6ba84d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Unmatched entries LlaMa 3.1: (0):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_fail(model, cleaned):\n",
    "\n",
    "    \"\"\"Print entries that failed, to manually correct these final entries\n",
    "    \"\"\"\n",
    "\n",
    "    failed_entries = [entry for entry in cleaned if fix_json(entry) is None]\n",
    "\n",
    "    print(f\"\\n\\n Unmatched entries {model}: ({len(failed_entries)}):\")\n",
    "    for entry in failed_entries:\n",
    "        print(entry)\n",
    "\n",
    "#print_fail('Qwen3-32B', output_qwen3_32B_cleaned)\n",
    "print_fail('LlaMa 3.1', output_llama31_cleaned)\n",
    "#print_fail('LlaMa 3.3', output_llama33_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a7889-1b9c-4409-ab3c-92d3d5552d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6ba1da5-077b-4017-9b1e-7ba31269668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually clean failed entries\n",
    "# manually_fixed = [{\"Prior_Emotion\": \"Fear\", \"Sentence\": \"I felt Fear when I was walking home alone through the empty streets after a late shift at the library, and I heard a sudden loud crash behind me, making me freeze in place, unsure if it was a threat or just the wind.\"}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2736ea91-8a20-4c5d-a142-a736bf2089ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qwen3_32B_results = pd.DataFrame(output_qwen3_32B_fixed)\n",
    "llama31_results = pd.DataFrame(output_llama31_fixed)\n",
    "#llama33_results = pd.DataFrame(output_llama33_fixed)\n",
    "\n",
    "\n",
    "llama31_results.to_csv('llama31_no_persona.csv')\n",
    "#llama33_results.to_csv('llama33_no_persona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d864a3b-cff3-40ff-a4e6-202c3b006981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I felt Fear when I was on a solo hike in the mountains and a sudden dense fog rolled in, reducing visibility to almost zero, making it impossible for me to see the trail or find my way back.\n",
      "I felt Shame when I accidentally posted a personal story on my professional social media account, which was seen by my colleagues and clients, causing me to feel deeply embarrassed and unprofessional.\n",
      "I felt Guilt when I convinced my younger sister to skip school and go to the mall with me, and afterwards she got in trouble with our parents for missing a crucial exam, which made me realize the gravity of my reckless suggestion.\n",
      "I felt Disgust when I stumbled upon a blog with outdated and poorly written content that was still trying to garner attention in the age of Instagram and TikTok, where short-form, visually-appealing posts have become the norm.\n",
      "I felt Sadness when I saw a photo of my grandfather, who had passed away a few years ago, and it brought back a flood of memories of the times we spent together and the stories he used to tell me, reminding me of the void his absence has left in my life.\n",
      "I felt Anger when a potential client dismissed my social media strategy for their business, stating that blogging was still the most effective way to reach their audience, despite all the data showing the opposite.\n",
      "I felt Joy when I saw my social media campaign for a local charity go viral and receive an overwhelming amount of support and donations from the community, exceeding our expectations and making a significant impact on the cause we were advocating for.\n"
     ]
    }
   ],
   "source": [
    "for idx, row in llama33_results.iterrows():\n",
    "    if 35 <= idx <= 41:\n",
    "        print(row['Sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe07ef-e7ed-4474-a842-4fc3183f86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_stats(results):\n",
    "\n",
    "    \"\"\"Check if model correctly produced samples of ~ 30 words.\n",
    "    \n",
    "    \"\"\"\n",
    "    results['Word_Count'] = results['Sentence'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    mean_length = results['Word_Count'].mean() \n",
    "    std_dev = results['Word_Count'].std()  \n",
    "    quartiles = np.percentile(results['Word_Count'], [25, 50, 75])\n",
    "    \n",
    "    print(f\"Mean Text Length: {mean_length:.2f} words\")\n",
    "    print(f\"Standard Deviation: {std_dev:.2f} words\")\n",
    "    print(f\"Quartiles (25th, 50th, 75th percentiles): {quartiles}\")\n",
    "    print(f\"Max length of words: {results['Word_Count'].max()}\")\n",
    "    print(f\"Min length of words: {results['Word_Count'].min()}\")\n",
    "\n",
    "#words_stats(qwen3_30BA3B_results)\n",
    "#words_stats(llama31_results)\n",
    "words_stats(llama33_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c6f2f0-490a-471e-b3d1-186e05da93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_emotions_and_i_felt(text):\n",
    "\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\b(shame|disgust|guilt|joy|sadness|anger|fear|ashamed|disgusted|guilty|joyful|angry|sad|scared)\\b\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\b(i felt|i feel|felt|feel)\\b\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text.encode().decode('unicode_escape')\n",
    "\n",
    "\n",
    "\n",
    "#qwen3_32B_results['Sentence'] = qwen3_32B_results['Sentence'].apply(remove_emotions_and_i_felt)\n",
    "\n",
    "llama31_results['Sentence'] = llama31_results['Sentence'].apply(remove_emotions_and_i_felt)\n",
    "\n",
    "#llama33_results['Sentence'] = llama33_results['Sentence'].apply(remove_emotions_and_i_felt)\n",
    "\n",
    "\n",
    "#qwen3_30BA3B_results['Sentence'] = qwen3_30BA3B_results['Sentence'].apply(remove_emotions_and_i_felt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b6f372-6bb9-4cfe-8216-c46b7887a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama31_results.to_csv('llama31_no_persona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb09c5-2d63-491d-bd3b-c00ca6790935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dupli_vaL_cts(results):\n",
    "\n",
    "    \"\"\"Function to check for duplicate values in LLM results\n",
    "    \"\"\"\n",
    "\n",
    "    duplicates = results[results['Sentence'].duplicated(keep=False)]\n",
    "    \n",
    "    print(\"\\n Duplicate Rows:\")\n",
    "    print(duplicates)\n",
    "    \n",
    "    print(\"\\nFull Text of Duplicate Sentences:\")\n",
    "    for idx, row in duplicates.iterrows():\n",
    "        print(f\"Index {idx}: {row['Sentence']}\")\n",
    "    \n",
    "    print(\"\\n Value Counts for 'Prior_Emotion':\")\n",
    "    print(results['Prior_Emotion'].value_counts())\n",
    "dupli_vaL_cts(llama33_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c904b384-43da-4b45-ac47-f02c13f99982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#qwen3_32B_results.to_csv('qwen3_32B_full_emotions.csv')\n",
    "#llama31_results.to_csv('llama31_full_emotions.csv')\n",
    "llama33_results.to_csv('llama33_full_emotions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
